{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "AtD4Q3d2QusY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XgtDs6eVWGBL",
        "outputId": "03356403-4466-4ee5-9719-caebffef5f75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[BernoulliRBM] Iteration 1, pseudo-likelihood = -26.00, time = 0.13s\n",
            "[BernoulliRBM] Iteration 2, pseudo-likelihood = -25.68, time = 0.29s\n",
            "[BernoulliRBM] Iteration 3, pseudo-likelihood = -25.18, time = 0.24s\n",
            "[BernoulliRBM] Iteration 4, pseudo-likelihood = -23.14, time = 0.21s\n",
            "[BernoulliRBM] Iteration 5, pseudo-likelihood = -21.82, time = 0.25s\n",
            "[BernoulliRBM] Iteration 6, pseudo-likelihood = -21.74, time = 0.29s\n",
            "[BernoulliRBM] Iteration 7, pseudo-likelihood = -21.47, time = 0.30s\n",
            "[BernoulliRBM] Iteration 8, pseudo-likelihood = -21.10, time = 0.20s\n",
            "[BernoulliRBM] Iteration 9, pseudo-likelihood = -20.80, time = 0.24s\n",
            "[BernoulliRBM] Iteration 10, pseudo-likelihood = -20.55, time = 0.24s\n",
            "[BernoulliRBM] Iteration 11, pseudo-likelihood = -20.25, time = 0.25s\n",
            "[BernoulliRBM] Iteration 12, pseudo-likelihood = -20.30, time = 0.17s\n",
            "[BernoulliRBM] Iteration 13, pseudo-likelihood = -20.00, time = 0.33s\n",
            "[BernoulliRBM] Iteration 14, pseudo-likelihood = -20.11, time = 0.29s\n",
            "[BernoulliRBM] Iteration 15, pseudo-likelihood = -19.80, time = 0.29s\n",
            "[BernoulliRBM] Iteration 16, pseudo-likelihood = -19.60, time = 0.23s\n",
            "[BernoulliRBM] Iteration 17, pseudo-likelihood = -19.60, time = 0.20s\n",
            "[BernoulliRBM] Iteration 18, pseudo-likelihood = -19.28, time = 0.22s\n",
            "[BernoulliRBM] Iteration 19, pseudo-likelihood = -19.41, time = 0.20s\n",
            "[BernoulliRBM] Iteration 20, pseudo-likelihood = -19.50, time = 0.21s\n",
            "[BernoulliRBM] Iteration 21, pseudo-likelihood = -19.44, time = 0.29s\n",
            "[BernoulliRBM] Iteration 22, pseudo-likelihood = -19.13, time = 0.23s\n",
            "[BernoulliRBM] Iteration 23, pseudo-likelihood = -18.56, time = 0.21s\n",
            "[BernoulliRBM] Iteration 24, pseudo-likelihood = -18.82, time = 0.20s\n",
            "[BernoulliRBM] Iteration 25, pseudo-likelihood = -18.66, time = 0.32s\n",
            "[BernoulliRBM] Iteration 26, pseudo-likelihood = -18.39, time = 0.27s\n",
            "[BernoulliRBM] Iteration 27, pseudo-likelihood = -18.49, time = 0.19s\n",
            "[BernoulliRBM] Iteration 28, pseudo-likelihood = -18.44, time = 0.14s\n",
            "[BernoulliRBM] Iteration 29, pseudo-likelihood = -18.23, time = 0.15s\n",
            "[BernoulliRBM] Iteration 30, pseudo-likelihood = -18.02, time = 0.14s\n",
            "[BernoulliRBM] Iteration 31, pseudo-likelihood = -18.12, time = 0.28s\n",
            "[BernoulliRBM] Iteration 32, pseudo-likelihood = -18.00, time = 0.26s\n",
            "[BernoulliRBM] Iteration 33, pseudo-likelihood = -18.07, time = 0.25s\n",
            "[BernoulliRBM] Iteration 34, pseudo-likelihood = -18.09, time = 0.34s\n",
            "[BernoulliRBM] Iteration 35, pseudo-likelihood = -18.01, time = 0.27s\n",
            "[BernoulliRBM] Iteration 36, pseudo-likelihood = -17.78, time = 0.21s\n",
            "[BernoulliRBM] Iteration 37, pseudo-likelihood = -17.75, time = 0.16s\n",
            "[BernoulliRBM] Iteration 38, pseudo-likelihood = -17.82, time = 0.18s\n",
            "[BernoulliRBM] Iteration 39, pseudo-likelihood = -17.88, time = 0.11s\n",
            "[BernoulliRBM] Iteration 40, pseudo-likelihood = -17.96, time = 0.18s\n",
            "[BernoulliRBM] Iteration 41, pseudo-likelihood = -17.61, time = 0.13s\n",
            "[BernoulliRBM] Iteration 42, pseudo-likelihood = -17.71, time = 0.16s\n",
            "[BernoulliRBM] Iteration 43, pseudo-likelihood = -17.63, time = 0.21s\n",
            "[BernoulliRBM] Iteration 44, pseudo-likelihood = -17.78, time = 0.20s\n",
            "[BernoulliRBM] Iteration 45, pseudo-likelihood = -17.52, time = 0.17s\n",
            "[BernoulliRBM] Iteration 46, pseudo-likelihood = -17.94, time = 0.16s\n",
            "[BernoulliRBM] Iteration 47, pseudo-likelihood = -17.53, time = 0.19s\n",
            "[BernoulliRBM] Iteration 48, pseudo-likelihood = -17.74, time = 0.25s\n",
            "[BernoulliRBM] Iteration 49, pseudo-likelihood = -17.41, time = 0.24s\n",
            "[BernoulliRBM] Iteration 50, pseudo-likelihood = -17.48, time = 0.21s\n",
            "[BernoulliRBM] Iteration 51, pseudo-likelihood = -17.39, time = 0.21s\n",
            "[BernoulliRBM] Iteration 52, pseudo-likelihood = -17.44, time = 0.30s\n",
            "[BernoulliRBM] Iteration 53, pseudo-likelihood = -17.68, time = 0.18s\n",
            "[BernoulliRBM] Iteration 54, pseudo-likelihood = -17.58, time = 0.19s\n",
            "[BernoulliRBM] Iteration 55, pseudo-likelihood = -17.55, time = 0.15s\n",
            "[BernoulliRBM] Iteration 56, pseudo-likelihood = -17.26, time = 0.23s\n",
            "[BernoulliRBM] Iteration 57, pseudo-likelihood = -17.27, time = 0.25s\n",
            "[BernoulliRBM] Iteration 58, pseudo-likelihood = -17.26, time = 0.20s\n",
            "[BernoulliRBM] Iteration 59, pseudo-likelihood = -17.24, time = 0.20s\n",
            "[BernoulliRBM] Iteration 60, pseudo-likelihood = -17.27, time = 0.21s\n",
            "[BernoulliRBM] Iteration 61, pseudo-likelihood = -17.17, time = 0.26s\n",
            "[BernoulliRBM] Iteration 62, pseudo-likelihood = -17.27, time = 0.29s\n",
            "[BernoulliRBM] Iteration 63, pseudo-likelihood = -17.30, time = 0.25s\n",
            "[BernoulliRBM] Iteration 64, pseudo-likelihood = -17.21, time = 0.28s\n",
            "[BernoulliRBM] Iteration 65, pseudo-likelihood = -17.24, time = 0.27s\n",
            "[BernoulliRBM] Iteration 66, pseudo-likelihood = -17.23, time = 0.33s\n",
            "[BernoulliRBM] Iteration 67, pseudo-likelihood = -17.38, time = 0.29s\n",
            "[BernoulliRBM] Iteration 68, pseudo-likelihood = -17.29, time = 0.33s\n",
            "[BernoulliRBM] Iteration 69, pseudo-likelihood = -17.03, time = 0.25s\n",
            "[BernoulliRBM] Iteration 70, pseudo-likelihood = -17.22, time = 0.37s\n",
            "[BernoulliRBM] Iteration 71, pseudo-likelihood = -16.90, time = 0.14s\n",
            "[BernoulliRBM] Iteration 72, pseudo-likelihood = -17.22, time = 0.27s\n",
            "[BernoulliRBM] Iteration 73, pseudo-likelihood = -17.03, time = 0.27s\n",
            "[BernoulliRBM] Iteration 74, pseudo-likelihood = -17.01, time = 0.33s\n",
            "[BernoulliRBM] Iteration 75, pseudo-likelihood = -17.14, time = 0.28s\n",
            "[BernoulliRBM] Iteration 76, pseudo-likelihood = -17.02, time = 0.19s\n",
            "[BernoulliRBM] Iteration 77, pseudo-likelihood = -16.96, time = 0.21s\n",
            "[BernoulliRBM] Iteration 78, pseudo-likelihood = -17.12, time = 0.23s\n",
            "[BernoulliRBM] Iteration 79, pseudo-likelihood = -17.00, time = 0.21s\n",
            "[BernoulliRBM] Iteration 80, pseudo-likelihood = -16.91, time = 0.27s\n",
            "[BernoulliRBM] Iteration 81, pseudo-likelihood = -17.02, time = 0.18s\n",
            "[BernoulliRBM] Iteration 82, pseudo-likelihood = -16.97, time = 0.26s\n",
            "[BernoulliRBM] Iteration 83, pseudo-likelihood = -17.01, time = 0.21s\n",
            "[BernoulliRBM] Iteration 84, pseudo-likelihood = -16.92, time = 0.14s\n",
            "[BernoulliRBM] Iteration 85, pseudo-likelihood = -17.05, time = 0.19s\n",
            "[BernoulliRBM] Iteration 86, pseudo-likelihood = -16.97, time = 0.14s\n",
            "[BernoulliRBM] Iteration 87, pseudo-likelihood = -16.85, time = 0.13s\n",
            "[BernoulliRBM] Iteration 88, pseudo-likelihood = -16.91, time = 0.12s\n",
            "[BernoulliRBM] Iteration 89, pseudo-likelihood = -17.01, time = 0.08s\n",
            "[BernoulliRBM] Iteration 90, pseudo-likelihood = -16.65, time = 0.07s\n",
            "[BernoulliRBM] Iteration 91, pseudo-likelihood = -17.04, time = 0.12s\n",
            "[BernoulliRBM] Iteration 92, pseudo-likelihood = -16.99, time = 0.07s\n",
            "[BernoulliRBM] Iteration 93, pseudo-likelihood = -16.85, time = 0.07s\n",
            "[BernoulliRBM] Iteration 94, pseudo-likelihood = -16.77, time = 0.05s\n",
            "[BernoulliRBM] Iteration 95, pseudo-likelihood = -16.82, time = 0.05s\n",
            "[BernoulliRBM] Iteration 96, pseudo-likelihood = -16.93, time = 0.05s\n",
            "[BernoulliRBM] Iteration 97, pseudo-likelihood = -16.95, time = 0.06s\n",
            "[BernoulliRBM] Iteration 98, pseudo-likelihood = -16.77, time = 0.06s\n",
            "[BernoulliRBM] Iteration 99, pseudo-likelihood = -16.96, time = 0.05s\n",
            "[BernoulliRBM] Iteration 100, pseudo-likelihood = -16.93, time = 0.05s\n",
            "[BernoulliRBM] Iteration 1, pseudo-likelihood = -15.45, time = 0.04s\n",
            "[BernoulliRBM] Iteration 2, pseudo-likelihood = -15.48, time = 0.06s\n",
            "[BernoulliRBM] Iteration 3, pseudo-likelihood = -15.45, time = 0.06s\n",
            "[BernoulliRBM] Iteration 4, pseudo-likelihood = -15.16, time = 0.06s\n",
            "[BernoulliRBM] Iteration 5, pseudo-likelihood = -15.02, time = 0.06s\n",
            "[BernoulliRBM] Iteration 6, pseudo-likelihood = -15.35, time = 0.06s\n",
            "[BernoulliRBM] Iteration 7, pseudo-likelihood = -14.92, time = 0.06s\n",
            "[BernoulliRBM] Iteration 8, pseudo-likelihood = -14.86, time = 0.06s\n",
            "[BernoulliRBM] Iteration 9, pseudo-likelihood = -14.67, time = 0.06s\n",
            "[BernoulliRBM] Iteration 10, pseudo-likelihood = -14.05, time = 0.06s\n",
            "[BernoulliRBM] Iteration 11, pseudo-likelihood = -13.24, time = 0.06s\n",
            "[BernoulliRBM] Iteration 12, pseudo-likelihood = -13.93, time = 0.06s\n",
            "[BernoulliRBM] Iteration 13, pseudo-likelihood = -13.41, time = 0.06s\n",
            "[BernoulliRBM] Iteration 14, pseudo-likelihood = -13.33, time = 0.06s\n",
            "[BernoulliRBM] Iteration 15, pseudo-likelihood = -12.71, time = 0.06s\n",
            "[BernoulliRBM] Iteration 16, pseudo-likelihood = -12.96, time = 0.06s\n",
            "[BernoulliRBM] Iteration 17, pseudo-likelihood = -12.88, time = 0.06s\n",
            "[BernoulliRBM] Iteration 18, pseudo-likelihood = -12.60, time = 0.06s\n",
            "[BernoulliRBM] Iteration 19, pseudo-likelihood = -12.68, time = 0.06s\n",
            "[BernoulliRBM] Iteration 20, pseudo-likelihood = -12.69, time = 0.06s\n",
            "[BernoulliRBM] Iteration 21, pseudo-likelihood = -12.29, time = 0.06s\n",
            "[BernoulliRBM] Iteration 22, pseudo-likelihood = -12.57, time = 0.09s\n",
            "[BernoulliRBM] Iteration 23, pseudo-likelihood = -12.55, time = 0.06s\n",
            "[BernoulliRBM] Iteration 24, pseudo-likelihood = -12.12, time = 0.06s\n",
            "[BernoulliRBM] Iteration 25, pseudo-likelihood = -12.22, time = 0.07s\n",
            "[BernoulliRBM] Iteration 26, pseudo-likelihood = -12.13, time = 0.06s\n",
            "[BernoulliRBM] Iteration 27, pseudo-likelihood = -12.37, time = 0.06s\n",
            "[BernoulliRBM] Iteration 28, pseudo-likelihood = -12.12, time = 0.06s\n",
            "[BernoulliRBM] Iteration 29, pseudo-likelihood = -11.73, time = 0.06s\n",
            "[BernoulliRBM] Iteration 30, pseudo-likelihood = -11.90, time = 0.06s\n",
            "[BernoulliRBM] Iteration 31, pseudo-likelihood = -12.15, time = 0.06s\n",
            "[BernoulliRBM] Iteration 32, pseudo-likelihood = -12.09, time = 0.06s\n",
            "[BernoulliRBM] Iteration 33, pseudo-likelihood = -12.56, time = 0.06s\n",
            "[BernoulliRBM] Iteration 34, pseudo-likelihood = -11.69, time = 0.06s\n",
            "[BernoulliRBM] Iteration 35, pseudo-likelihood = -11.52, time = 0.06s\n",
            "[BernoulliRBM] Iteration 36, pseudo-likelihood = -12.18, time = 0.06s\n",
            "[BernoulliRBM] Iteration 37, pseudo-likelihood = -11.38, time = 0.06s\n",
            "[BernoulliRBM] Iteration 38, pseudo-likelihood = -11.61, time = 0.06s\n",
            "[BernoulliRBM] Iteration 39, pseudo-likelihood = -11.85, time = 0.06s\n",
            "[BernoulliRBM] Iteration 40, pseudo-likelihood = -11.40, time = 0.06s\n",
            "[BernoulliRBM] Iteration 41, pseudo-likelihood = -12.02, time = 0.06s\n",
            "[BernoulliRBM] Iteration 42, pseudo-likelihood = -10.94, time = 0.06s\n",
            "[BernoulliRBM] Iteration 43, pseudo-likelihood = -11.55, time = 0.08s\n",
            "[BernoulliRBM] Iteration 44, pseudo-likelihood = -11.53, time = 0.06s\n",
            "[BernoulliRBM] Iteration 45, pseudo-likelihood = -11.63, time = 0.06s\n",
            "[BernoulliRBM] Iteration 46, pseudo-likelihood = -11.46, time = 0.06s\n",
            "[BernoulliRBM] Iteration 47, pseudo-likelihood = -11.47, time = 0.06s\n",
            "[BernoulliRBM] Iteration 48, pseudo-likelihood = -11.50, time = 0.06s\n",
            "[BernoulliRBM] Iteration 49, pseudo-likelihood = -11.82, time = 0.06s\n",
            "[BernoulliRBM] Iteration 50, pseudo-likelihood = -11.33, time = 0.06s\n",
            "[BernoulliRBM] Iteration 51, pseudo-likelihood = -11.39, time = 0.06s\n",
            "[BernoulliRBM] Iteration 52, pseudo-likelihood = -11.90, time = 0.06s\n",
            "[BernoulliRBM] Iteration 53, pseudo-likelihood = -11.59, time = 0.06s\n",
            "[BernoulliRBM] Iteration 54, pseudo-likelihood = -11.23, time = 0.06s\n",
            "[BernoulliRBM] Iteration 55, pseudo-likelihood = -11.38, time = 0.06s\n",
            "[BernoulliRBM] Iteration 56, pseudo-likelihood = -11.78, time = 0.06s\n",
            "[BernoulliRBM] Iteration 57, pseudo-likelihood = -11.20, time = 0.06s\n",
            "[BernoulliRBM] Iteration 58, pseudo-likelihood = -11.48, time = 0.06s\n",
            "[BernoulliRBM] Iteration 59, pseudo-likelihood = -11.21, time = 0.05s\n",
            "[BernoulliRBM] Iteration 60, pseudo-likelihood = -11.17, time = 0.06s\n",
            "[BernoulliRBM] Iteration 61, pseudo-likelihood = -11.20, time = 0.08s\n",
            "[BernoulliRBM] Iteration 62, pseudo-likelihood = -11.19, time = 0.05s\n",
            "[BernoulliRBM] Iteration 63, pseudo-likelihood = -11.12, time = 0.06s\n",
            "[BernoulliRBM] Iteration 64, pseudo-likelihood = -11.61, time = 0.06s\n",
            "[BernoulliRBM] Iteration 65, pseudo-likelihood = -11.25, time = 0.06s\n",
            "[BernoulliRBM] Iteration 66, pseudo-likelihood = -11.35, time = 0.06s\n",
            "[BernoulliRBM] Iteration 67, pseudo-likelihood = -11.28, time = 0.06s\n",
            "[BernoulliRBM] Iteration 68, pseudo-likelihood = -11.44, time = 0.06s\n",
            "[BernoulliRBM] Iteration 69, pseudo-likelihood = -10.96, time = 0.06s\n",
            "[BernoulliRBM] Iteration 70, pseudo-likelihood = -11.30, time = 0.06s\n",
            "[BernoulliRBM] Iteration 71, pseudo-likelihood = -11.26, time = 0.06s\n",
            "[BernoulliRBM] Iteration 72, pseudo-likelihood = -11.10, time = 0.06s\n",
            "[BernoulliRBM] Iteration 73, pseudo-likelihood = -11.18, time = 0.05s\n",
            "[BernoulliRBM] Iteration 74, pseudo-likelihood = -11.28, time = 0.06s\n",
            "[BernoulliRBM] Iteration 75, pseudo-likelihood = -10.93, time = 0.05s\n",
            "[BernoulliRBM] Iteration 76, pseudo-likelihood = -11.17, time = 0.06s\n",
            "[BernoulliRBM] Iteration 77, pseudo-likelihood = -11.15, time = 0.06s\n",
            "[BernoulliRBM] Iteration 78, pseudo-likelihood = -10.89, time = 0.06s\n",
            "[BernoulliRBM] Iteration 79, pseudo-likelihood = -11.17, time = 0.06s\n",
            "[BernoulliRBM] Iteration 80, pseudo-likelihood = -11.13, time = 0.06s\n",
            "[BernoulliRBM] Iteration 81, pseudo-likelihood = -11.20, time = 0.05s\n",
            "[BernoulliRBM] Iteration 82, pseudo-likelihood = -11.19, time = 0.06s\n",
            "[BernoulliRBM] Iteration 83, pseudo-likelihood = -11.08, time = 0.06s\n",
            "[BernoulliRBM] Iteration 84, pseudo-likelihood = -11.02, time = 0.06s\n",
            "[BernoulliRBM] Iteration 85, pseudo-likelihood = -11.00, time = 0.06s\n",
            "[BernoulliRBM] Iteration 86, pseudo-likelihood = -11.00, time = 0.05s\n",
            "[BernoulliRBM] Iteration 87, pseudo-likelihood = -11.10, time = 0.05s\n",
            "[BernoulliRBM] Iteration 88, pseudo-likelihood = -11.16, time = 0.06s\n",
            "[BernoulliRBM] Iteration 89, pseudo-likelihood = -10.92, time = 0.05s\n",
            "[BernoulliRBM] Iteration 90, pseudo-likelihood = -11.04, time = 0.05s\n",
            "[BernoulliRBM] Iteration 91, pseudo-likelihood = -11.25, time = 0.05s\n",
            "[BernoulliRBM] Iteration 92, pseudo-likelihood = -11.19, time = 0.06s\n",
            "[BernoulliRBM] Iteration 93, pseudo-likelihood = -11.10, time = 0.05s\n",
            "[BernoulliRBM] Iteration 94, pseudo-likelihood = -10.78, time = 0.06s\n",
            "[BernoulliRBM] Iteration 95, pseudo-likelihood = -11.03, time = 0.05s\n",
            "[BernoulliRBM] Iteration 96, pseudo-likelihood = -11.06, time = 0.07s\n",
            "[BernoulliRBM] Iteration 97, pseudo-likelihood = -10.96, time = 0.06s\n",
            "[BernoulliRBM] Iteration 98, pseudo-likelihood = -11.13, time = 0.05s\n",
            "[BernoulliRBM] Iteration 99, pseudo-likelihood = -10.98, time = 0.06s\n",
            "[BernoulliRBM] Iteration 100, pseudo-likelihood = -10.91, time = 0.06s\n",
            "[BernoulliRBM] Iteration 1, pseudo-likelihood = -30.81, time = 0.03s\n",
            "[BernoulliRBM] Iteration 2, pseudo-likelihood = -30.39, time = 0.04s\n",
            "[BernoulliRBM] Iteration 3, pseudo-likelihood = -30.79, time = 0.04s\n",
            "[BernoulliRBM] Iteration 4, pseudo-likelihood = -30.61, time = 0.04s\n",
            "[BernoulliRBM] Iteration 5, pseudo-likelihood = -30.34, time = 0.05s\n",
            "[BernoulliRBM] Iteration 6, pseudo-likelihood = -30.55, time = 0.04s\n",
            "[BernoulliRBM] Iteration 7, pseudo-likelihood = -29.75, time = 0.05s\n",
            "[BernoulliRBM] Iteration 8, pseudo-likelihood = -29.61, time = 0.04s\n",
            "[BernoulliRBM] Iteration 9, pseudo-likelihood = -28.98, time = 0.04s\n",
            "[BernoulliRBM] Iteration 10, pseudo-likelihood = -29.08, time = 0.05s\n",
            "[BernoulliRBM] Iteration 11, pseudo-likelihood = -29.05, time = 0.04s\n",
            "[BernoulliRBM] Iteration 12, pseudo-likelihood = -28.67, time = 0.04s\n",
            "[BernoulliRBM] Iteration 13, pseudo-likelihood = -28.84, time = 0.04s\n",
            "[BernoulliRBM] Iteration 14, pseudo-likelihood = -28.61, time = 0.05s\n",
            "[BernoulliRBM] Iteration 15, pseudo-likelihood = -28.34, time = 0.05s\n",
            "[BernoulliRBM] Iteration 16, pseudo-likelihood = -28.24, time = 0.05s\n",
            "[BernoulliRBM] Iteration 17, pseudo-likelihood = -28.53, time = 0.04s\n",
            "[BernoulliRBM] Iteration 18, pseudo-likelihood = -27.85, time = 0.06s\n",
            "[BernoulliRBM] Iteration 19, pseudo-likelihood = -28.03, time = 0.04s\n",
            "[BernoulliRBM] Iteration 20, pseudo-likelihood = -27.82, time = 0.05s\n",
            "[BernoulliRBM] Iteration 21, pseudo-likelihood = -28.10, time = 0.05s\n",
            "[BernoulliRBM] Iteration 22, pseudo-likelihood = -28.12, time = 0.05s\n",
            "[BernoulliRBM] Iteration 23, pseudo-likelihood = -27.91, time = 0.04s\n",
            "[BernoulliRBM] Iteration 24, pseudo-likelihood = -27.73, time = 0.04s\n",
            "[BernoulliRBM] Iteration 25, pseudo-likelihood = -28.28, time = 0.05s\n",
            "[BernoulliRBM] Iteration 26, pseudo-likelihood = -27.92, time = 0.04s\n",
            "[BernoulliRBM] Iteration 27, pseudo-likelihood = -27.74, time = 0.04s\n",
            "[BernoulliRBM] Iteration 28, pseudo-likelihood = -27.79, time = 0.04s\n",
            "[BernoulliRBM] Iteration 29, pseudo-likelihood = -27.58, time = 0.04s\n",
            "[BernoulliRBM] Iteration 30, pseudo-likelihood = -27.94, time = 0.05s\n",
            "[BernoulliRBM] Iteration 31, pseudo-likelihood = -27.79, time = 0.04s\n",
            "[BernoulliRBM] Iteration 32, pseudo-likelihood = -27.89, time = 0.04s\n",
            "[BernoulliRBM] Iteration 33, pseudo-likelihood = -27.63, time = 0.04s\n",
            "[BernoulliRBM] Iteration 34, pseudo-likelihood = -27.84, time = 0.04s\n",
            "[BernoulliRBM] Iteration 35, pseudo-likelihood = -28.00, time = 0.05s\n",
            "[BernoulliRBM] Iteration 36, pseudo-likelihood = -27.93, time = 0.05s\n",
            "[BernoulliRBM] Iteration 37, pseudo-likelihood = -27.44, time = 0.04s\n",
            "[BernoulliRBM] Iteration 38, pseudo-likelihood = -27.60, time = 0.04s\n",
            "[BernoulliRBM] Iteration 39, pseudo-likelihood = -27.44, time = 0.04s\n",
            "[BernoulliRBM] Iteration 40, pseudo-likelihood = -27.57, time = 0.05s\n",
            "[BernoulliRBM] Iteration 41, pseudo-likelihood = -27.60, time = 0.05s\n",
            "[BernoulliRBM] Iteration 42, pseudo-likelihood = -27.76, time = 0.05s\n",
            "[BernoulliRBM] Iteration 43, pseudo-likelihood = -27.82, time = 0.04s\n",
            "[BernoulliRBM] Iteration 44, pseudo-likelihood = -27.59, time = 0.05s\n",
            "[BernoulliRBM] Iteration 45, pseudo-likelihood = -27.38, time = 0.07s\n",
            "[BernoulliRBM] Iteration 46, pseudo-likelihood = -27.53, time = 0.09s\n",
            "[BernoulliRBM] Iteration 47, pseudo-likelihood = -27.55, time = 0.05s\n",
            "[BernoulliRBM] Iteration 48, pseudo-likelihood = -27.53, time = 0.09s\n",
            "[BernoulliRBM] Iteration 49, pseudo-likelihood = -27.49, time = 0.05s\n",
            "[BernoulliRBM] Iteration 50, pseudo-likelihood = -27.43, time = 0.09s\n",
            "[BernoulliRBM] Iteration 51, pseudo-likelihood = -27.42, time = 0.12s\n",
            "[BernoulliRBM] Iteration 52, pseudo-likelihood = -27.23, time = 0.12s\n",
            "[BernoulliRBM] Iteration 53, pseudo-likelihood = -27.29, time = 0.10s\n",
            "[BernoulliRBM] Iteration 54, pseudo-likelihood = -27.15, time = 0.14s\n",
            "[BernoulliRBM] Iteration 55, pseudo-likelihood = -27.37, time = 0.12s\n",
            "[BernoulliRBM] Iteration 56, pseudo-likelihood = -27.23, time = 0.12s\n",
            "[BernoulliRBM] Iteration 57, pseudo-likelihood = -27.30, time = 0.14s\n",
            "[BernoulliRBM] Iteration 58, pseudo-likelihood = -27.28, time = 0.09s\n",
            "[BernoulliRBM] Iteration 59, pseudo-likelihood = -27.36, time = 0.14s\n",
            "[BernoulliRBM] Iteration 60, pseudo-likelihood = -27.16, time = 0.13s\n",
            "[BernoulliRBM] Iteration 61, pseudo-likelihood = -27.24, time = 0.14s\n",
            "[BernoulliRBM] Iteration 62, pseudo-likelihood = -27.36, time = 0.13s\n",
            "[BernoulliRBM] Iteration 63, pseudo-likelihood = -27.19, time = 0.08s\n",
            "[BernoulliRBM] Iteration 64, pseudo-likelihood = -27.21, time = 0.06s\n",
            "[BernoulliRBM] Iteration 65, pseudo-likelihood = -27.40, time = 0.05s\n",
            "[BernoulliRBM] Iteration 66, pseudo-likelihood = -27.11, time = 0.07s\n",
            "[BernoulliRBM] Iteration 67, pseudo-likelihood = -27.20, time = 0.14s\n",
            "[BernoulliRBM] Iteration 68, pseudo-likelihood = -27.12, time = 0.10s\n",
            "[BernoulliRBM] Iteration 69, pseudo-likelihood = -27.02, time = 0.13s\n",
            "[BernoulliRBM] Iteration 70, pseudo-likelihood = -27.01, time = 0.09s\n",
            "[BernoulliRBM] Iteration 71, pseudo-likelihood = -26.97, time = 0.06s\n",
            "[BernoulliRBM] Iteration 72, pseudo-likelihood = -27.12, time = 0.06s\n",
            "[BernoulliRBM] Iteration 73, pseudo-likelihood = -27.00, time = 0.10s\n",
            "[BernoulliRBM] Iteration 74, pseudo-likelihood = -27.13, time = 0.07s\n",
            "[BernoulliRBM] Iteration 75, pseudo-likelihood = -26.90, time = 0.07s\n",
            "[BernoulliRBM] Iteration 76, pseudo-likelihood = -27.20, time = 0.04s\n",
            "[BernoulliRBM] Iteration 77, pseudo-likelihood = -26.90, time = 0.04s\n",
            "[BernoulliRBM] Iteration 78, pseudo-likelihood = -26.80, time = 0.05s\n",
            "[BernoulliRBM] Iteration 79, pseudo-likelihood = -27.09, time = 0.04s\n",
            "[BernoulliRBM] Iteration 80, pseudo-likelihood = -26.93, time = 0.04s\n",
            "[BernoulliRBM] Iteration 81, pseudo-likelihood = -27.10, time = 0.04s\n",
            "[BernoulliRBM] Iteration 82, pseudo-likelihood = -27.11, time = 0.04s\n",
            "[BernoulliRBM] Iteration 83, pseudo-likelihood = -26.94, time = 0.05s\n",
            "[BernoulliRBM] Iteration 84, pseudo-likelihood = -26.85, time = 0.04s\n",
            "[BernoulliRBM] Iteration 85, pseudo-likelihood = -26.80, time = 0.04s\n",
            "[BernoulliRBM] Iteration 86, pseudo-likelihood = -26.95, time = 0.04s\n",
            "[BernoulliRBM] Iteration 87, pseudo-likelihood = -26.94, time = 0.04s\n",
            "[BernoulliRBM] Iteration 88, pseudo-likelihood = -26.84, time = 0.05s\n",
            "[BernoulliRBM] Iteration 89, pseudo-likelihood = -26.94, time = 0.04s\n",
            "[BernoulliRBM] Iteration 90, pseudo-likelihood = -26.94, time = 0.04s\n",
            "[BernoulliRBM] Iteration 91, pseudo-likelihood = -26.65, time = 0.05s\n",
            "[BernoulliRBM] Iteration 92, pseudo-likelihood = -27.10, time = 0.10s\n",
            "[BernoulliRBM] Iteration 93, pseudo-likelihood = -27.03, time = 0.06s\n",
            "[BernoulliRBM] Iteration 94, pseudo-likelihood = -26.94, time = 0.17s\n",
            "[BernoulliRBM] Iteration 95, pseudo-likelihood = -26.83, time = 0.05s\n",
            "[BernoulliRBM] Iteration 96, pseudo-likelihood = -26.94, time = 0.05s\n",
            "[BernoulliRBM] Iteration 97, pseudo-likelihood = -26.95, time = 0.05s\n",
            "[BernoulliRBM] Iteration 98, pseudo-likelihood = -26.72, time = 0.15s\n",
            "[BernoulliRBM] Iteration 99, pseudo-likelihood = -26.94, time = 0.14s\n",
            "[BernoulliRBM] Iteration 100, pseudo-likelihood = -26.79, time = 0.11s\n",
            "Logistic regression using RBM features:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        27\n",
            "           1       0.91      0.83      0.87        35\n",
            "           2       0.88      0.83      0.86        36\n",
            "           3       0.66      0.93      0.77        29\n",
            "           4       1.00      0.93      0.97        30\n",
            "           5       0.95      0.90      0.92        40\n",
            "           6       1.00      0.95      0.98        44\n",
            "           7       0.90      0.97      0.94        39\n",
            "           8       0.89      0.87      0.88        39\n",
            "           9       0.76      0.71      0.73        41\n",
            "\n",
            "    accuracy                           0.89       360\n",
            "   macro avg       0.90      0.89      0.89       360\n",
            "weighted avg       0.90      0.89      0.89       360\n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ],
      "source": [
        "from sklearn.neural_network import BernoulliRBM\n",
        "import numpy as np\n",
        "from sklearn import linear_model, datasets, metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "digits = datasets.load_digits()\n",
        "X = np.asarray(digits.data, 'float32')\n",
        "Y = digits.target\n",
        "X = (X - np.min(X, 0)) / (np.max(X, 0) + 0.0001)  # 0-1 scaling\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y,\n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state=0)\n",
        "\n",
        "logistic = linear_model.LogisticRegression(C=100)\n",
        "rbm1 = BernoulliRBM(n_components=100, learning_rate=0.06, n_iter=100, verbose=1, random_state=101)\n",
        "rbm2 = BernoulliRBM(n_components=80, learning_rate=0.06, n_iter=100, verbose=1, random_state=101)\n",
        "rbm3 = BernoulliRBM(n_components=60, learning_rate=0.06, n_iter=100, verbose=1, random_state=101)\n",
        "DBN3 = Pipeline(steps=[('rbm1', rbm1),('rbm2', rbm2), ('rbm3', rbm3), ('logistic', logistic)])\n",
        "\n",
        "DBN3.fit(X_train, Y_train)\n",
        "\n",
        "print(\"Logistic regression using RBM features:\\n%s\\n\" % (\n",
        "    metrics.classification_report(\n",
        "        Y_test,\n",
        "        DBN3.predict(X_test))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "id": "lFkTAQolXdlZ",
        "outputId": "05c14c89-4768-49e7-d503-002bb740e0f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pretraining RBM Layer 1\n",
            "Pretraining RBM Layer 2\n",
            "Pretraining RBM Layer 3\n",
            "Epoch 1 Loss: 1792.5045\n",
            "Epoch 2 Loss: 1789.1655\n",
            "Epoch 3 Loss: 1789.3951\n",
            "Epoch 4 Loss: 1789.2199\n",
            "Epoch 5 Loss: 1789.2327\n",
            "\n",
            "[Feature Visualization using t-SNE]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-f9bddef8f540>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n[Feature Visualization using t-SNE]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdbn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m \u001b[0mplot_tsne\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"DBN Representation (t-SNE)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n[Feature Visualization using PCA]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-f9bddef8f540>\u001b[0m in \u001b[0;36mplot_tsne\u001b[0;34m(features, labels, title)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot_tsne\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"t-SNE\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0mtsne\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTSNE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperplexity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pca'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m     \u001b[0mreduced\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtsne\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatterplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreduced\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreduced\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpalette\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tab10'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlegend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'full'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/manifold/_t_sne.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   1176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_params_vs_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m         \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1179\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/manifold/_t_sne.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, skip_num_points)\u001b[0m\n\u001b[1;32m   1044\u001b[0m         \u001b[0mdegrees_of_freedom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_components\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1046\u001b[0;31m         return self._tsne(\n\u001b[0m\u001b[1;32m   1047\u001b[0m             \u001b[0mP\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m             \u001b[0mdegrees_of_freedom\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/manifold/_t_sne.py\u001b[0m in \u001b[0;36m_tsne\u001b[0;34m(self, P, degrees_of_freedom, n_samples, X_embedded, neighbors, skip_num_points)\u001b[0m\n\u001b[1;32m   1112\u001b[0m             \u001b[0mopt_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"momentum\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m             \u001b[0mopt_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"n_iter_without_progress\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter_without_progress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkl_divergence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_gradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mopt_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m         \u001b[0;31m# Save the final number of iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/manifold/_t_sne.py\u001b[0m in \u001b[0;36m_gradient_descent\u001b[0;34m(objective, p0, it, max_iter, n_iter_check, n_iter_without_progress, momentum, learning_rate, min_gain, min_grad_norm, verbose, args, kwargs)\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"compute_error\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_convergence\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmax_iter\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m         \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobjective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m         \u001b[0minc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/manifold/_t_sne.py\u001b[0m in \u001b[0;36m_kl_divergence_bh\u001b[0;34m(params, P, degrees_of_freedom, n_samples, n_components, angle, skip_num_points, verbose, compute_error, num_threads)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_embedded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m     error = _barnes_hut_tsne.gradient(\n\u001b[0m\u001b[1;32m    283\u001b[0m         \u001b[0mval_P\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0mX_embedded\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "\n",
        "# Config\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 5\n",
        "\n",
        "# Transform: Convert to grayscale + normalize\n",
        "transform = transforms.Compose([\n",
        "    transforms.Grayscale(),  # Converts RGB -> 1 channel\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Load CIFAR10\n",
        "train_dataset = CIFAR10(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = CIFAR10(root='./data', train=False, transform=transform, download=True)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# ======================= RBM ======================= #\n",
        "class RBM(nn.Module):\n",
        "    def __init__(self, n_vis, n_hid):\n",
        "        super(RBM, self).__init__()\n",
        "        self.W = nn.Parameter(torch.randn(n_hid, n_vis) * 0.01)\n",
        "        self.v_bias = nn.Parameter(torch.zeros(n_vis))\n",
        "        self.h_bias = nn.Parameter(torch.zeros(n_hid))\n",
        "\n",
        "    def sample_h(self, v):\n",
        "        prob = torch.sigmoid(F.linear(v, self.W, self.h_bias))\n",
        "        return prob.bernoulli(), prob\n",
        "\n",
        "    def sample_v(self, h):\n",
        "        prob = torch.sigmoid(F.linear(h, self.W.t(), self.v_bias))\n",
        "        return prob.bernoulli(), prob\n",
        "\n",
        "    def contrastive_divergence(self, v0, k=1, lr=0.1):\n",
        "        vk = v0\n",
        "        for _ in range(k):\n",
        "            hk, _ = self.sample_h(vk)\n",
        "            vk, _ = self.sample_v(hk)\n",
        "        h0_prob = torch.sigmoid(F.linear(v0, self.W, self.h_bias))\n",
        "        hk_prob = torch.sigmoid(F.linear(vk, self.W, self.h_bias))\n",
        "        self.W.data += lr * (h0_prob.t() @ v0 - hk_prob.t() @ vk) / v0.size(0)\n",
        "        self.v_bias.data += lr * torch.mean(v0 - vk, dim=0)\n",
        "        self.h_bias.data += lr * torch.mean(h0_prob - hk_prob, dim=0)\n",
        "\n",
        "# ======================= DBN ======================= #\n",
        "class DBN(nn.Module):\n",
        "    def __init__(self, layers):\n",
        "        super(DBN, self).__init__()\n",
        "        self.rbms = nn.ModuleList([RBM(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
        "        self.classifier = nn.Linear(layers[-1], 10)\n",
        "\n",
        "    def pretrain(self, data_loader, epochs=5):\n",
        "        input_data = []\n",
        "        for idx, rbm in enumerate(self.rbms):\n",
        "            print(f\"Pretraining RBM Layer {idx+1}\")\n",
        "            for epoch in range(epochs):\n",
        "                for x, _ in data_loader:\n",
        "                    x = x.view(x.size(0), -1).to(device)\n",
        "                    if input_data:\n",
        "                        for prev_rbm in input_data:\n",
        "                          x = torch.sigmoid(F.linear(x, prev_rbm.W, prev_rbm.h_bias))\n",
        "                    rbm.contrastive_divergence(x)\n",
        "            # Get the output transformation\n",
        "            input_data.append(rbm)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        for rbm in self.rbms:\n",
        "            x = torch.sigmoid(F.linear(x, rbm.W, rbm.h_bias))\n",
        "        return self.classifier(x)\n",
        "\n",
        "# =================== Traditional Deep Net =================== #\n",
        "class DeepNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DeepNet, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(1024, 512), nn.ReLU(),\n",
        "            nn.Linear(512, 256), nn.ReLU(),\n",
        "            nn.Linear(256, 128), nn.ReLU(),\n",
        "            nn.Linear(128, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return self.net(x)\n",
        "\n",
        "# ==================== Train & Evaluate ==================== #\n",
        "def train(model, loader, epochs=5):\n",
        "    model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            out = model(x)\n",
        "            loss = criterion(out, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1} Loss: {total_loss:.4f}\")\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            out = model(x)\n",
        "            _, pred = torch.max(out, 1)\n",
        "            correct += (pred == y).sum().item()\n",
        "            total += y.size(0)\n",
        "    acc = 100 * correct / total\n",
        "    print(f\"Accuracy: {acc:.2f}%\")\n",
        "    return acc\n",
        "\n",
        "# ================== Run ================== #\n",
        "dbn = DBN([1024, 512, 256, 128]).to(device)\n",
        "dbn.pretrain(train_loader, epochs=3)\n",
        "train(dbn, train_loader, epochs=5)\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def extract_features(model, loader):\n",
        "    model.eval()\n",
        "    features = []\n",
        "    labels = []\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x = x.view(x.size(0), -1).to(device)\n",
        "            for rbm in model.rbms:\n",
        "                x = torch.sigmoid(F.linear(x, rbm.W, rbm.h_bias))\n",
        "            features.append(x.cpu())\n",
        "            labels.append(y)\n",
        "    return torch.cat(features), torch.cat(labels)\n",
        "\n",
        "def plot_tsne(features, labels, title=\"t-SNE\"):\n",
        "    tsne = TSNE(n_components=2, perplexity=30, init='pca', learning_rate='auto')\n",
        "    reduced = tsne.fit_transform(features)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.scatterplot(x=reduced[:, 0], y=reduced[:, 1], hue=labels, palette='tab10', s=40, legend='full')\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "def plot_pca(features, labels, title=\"PCA\"):\n",
        "    pca = PCA(n_components=2)\n",
        "    reduced = pca.fit_transform(features)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.scatterplot(x=reduced[:, 0], y=reduced[:, 1], hue=labels, palette='tab10', s=40, legend='full')\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "# ========== Visualize DBN representations ==========\n",
        "print(\"\\n[Feature Visualization using t-SNE]\")\n",
        "features, labels = extract_features(dbn, test_loader)\n",
        "plot_tsne(features.numpy(), labels.numpy(), title=\"DBN Representation (t-SNE)\")\n",
        "\n",
        "print(\"\\n[Feature Visualization using PCA]\")\n",
        "plot_pca(features.numpy(), labels.numpy(), title=\"DBN Representation (PCA)\")\n",
        "\n",
        "print(\"\\n[DBN Evaluation]\")\n",
        "evaluate(dbn, test_loader)\n",
        "\n",
        "print(\"\\nTraining DeepNet for comparison...\")\n",
        "deepnet = DeepNet().to(device)\n",
        "train(deepnet, train_loader, epochs=5)\n",
        "print(\"\\n[DeepNet Evaluation]\")\n",
        "evaluate(deepnet, test_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRCT74JcWR4U",
        "outputId": "faad4512-93ee-449d-bdfe-ab886a8c7bc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.11/dist-packages (from seaborn) (2.2.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m83.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "pip install torch torchvision matplotlib scikit-learn seaborn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3_AylTDXbCD",
        "outputId": "cfbadfa1-0c0a-4487-b3fb-90de46427d21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pretraining RBM Layer 1\n",
            "Pretraining RBM Layer 2\n",
            "Pretraining RBM Layer 3\n",
            "Epoch 1/5 - Loss: 1794.9199\n",
            "Epoch 2/5 - Loss: 1790.5410\n",
            "Epoch 3/5 - Loss: 1789.4708\n",
            "Epoch 4/5 - Loss: 1788.1126\n",
            "Epoch 5/5 - Loss: 1786.6670\n",
            "\n",
            "[Evaluation on Test Set]\n",
            "Accuracy: 13.80%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision import transforms\n",
        "\n",
        "# ---------------------------- Config ---------------------------- #\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS_PRETRAIN = 5\n",
        "EPOCHS_FINETUNE = 5\n",
        "INPUT_SIZE = 32 * 32  # grayscale\n",
        "\n",
        "# --------------------- Load CIFAR-10 (grayscale) --------------------- #\n",
        "transform = transforms.Compose([\n",
        "    transforms.Grayscale(),        # Convert RGB to 1 channel\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "train_set = CIFAR10(root='./data', train=True, transform=transform, download=True)\n",
        "test_set = CIFAR10(root='./data', train=False, transform=transform, download=True)\n",
        "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE)\n",
        "\n",
        "# ----------------------------- RBM ----------------------------- #\n",
        "class RBM(nn.Module):\n",
        "    def __init__(self, n_vis, n_hid):\n",
        "        super(RBM, self).__init__()\n",
        "        self.W = nn.Parameter(torch.randn(n_hid, n_vis) * 0.01)\n",
        "        self.v_bias = nn.Parameter(torch.zeros(n_vis))\n",
        "        self.h_bias = nn.Parameter(torch.zeros(n_hid))\n",
        "\n",
        "    def sample_h(self, v):\n",
        "        prob = torch.sigmoid(F.linear(v, self.W, self.h_bias))\n",
        "        return prob.bernoulli(), prob\n",
        "\n",
        "    def sample_v(self, h):\n",
        "        prob = torch.sigmoid(F.linear(h, self.W.t(), self.v_bias))\n",
        "        return prob.bernoulli(), prob\n",
        "\n",
        "    def contrastive_divergence(self, v0, lr=0.1, k=1):\n",
        "        vk = v0\n",
        "        for _ in range(k):\n",
        "            hk, _ = self.sample_h(vk)\n",
        "            vk, _ = self.sample_v(hk)\n",
        "        h0_prob = torch.sigmoid(F.linear(v0, self.W, self.h_bias))\n",
        "        hk_prob = torch.sigmoid(F.linear(vk, self.W, self.h_bias))\n",
        "\n",
        "        self.W.data += lr * (h0_prob.t() @ v0 - hk_prob.t() @ vk) / v0.size(0)\n",
        "        self.v_bias.data += lr * torch.mean(v0 - vk, dim=0)\n",
        "        self.h_bias.data += lr * torch.mean(h0_prob - hk_prob, dim=0)\n",
        "\n",
        "# ------------------------- Deep Belief Network ------------------------- #\n",
        "class DBN(nn.Module):\n",
        "    def __init__(self, layer_sizes):\n",
        "        super(DBN, self).__init__()\n",
        "        self.rbms = nn.ModuleList([RBM(layer_sizes[i], layer_sizes[i+1]) for i in range(len(layer_sizes)-1)])\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(layer_sizes[-1], 10)\n",
        "        )\n",
        "\n",
        "    def pretrain(self, loader, epochs):\n",
        "        data = None\n",
        "        for idx, rbm in enumerate(self.rbms):\n",
        "            print(f\"Pretraining RBM Layer {idx+1}\")\n",
        "            for epoch in range(epochs):\n",
        "                for x, _ in loader:\n",
        "                    x = x.view(x.size(0), -1).to(device)\n",
        "                    if data:\n",
        "                        for prev_rbm in data:\n",
        "                            x = torch.sigmoid(F.linear(x, prev_rbm.W, prev_rbm.h_bias))\n",
        "                    rbm.contrastive_divergence(x)\n",
        "            data = data or []\n",
        "            data.append(rbm)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        for rbm in self.rbms:\n",
        "            x = torch.sigmoid(F.linear(x, rbm.W, rbm.h_bias))\n",
        "        return self.classifier(x)\n",
        "\n",
        "# --------------------- Training & Evaluation --------------------- #\n",
        "def train_supervised(model, loader, epochs):\n",
        "    model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(x)\n",
        "            loss = criterion(output, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {total_loss:.4f}\")\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            outputs = model(x)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += y.size(0)\n",
        "            correct += (predicted == y).sum().item()\n",
        "    print(f\"Accuracy: {100 * correct / total:.2f}%\")\n",
        "\n",
        "# ---------------------- Build and Run ---------------------- #\n",
        "layer_sizes = [INPUT_SIZE, 512, 256, 128]\n",
        "dbn = DBN(layer_sizes).to(device)\n",
        "\n",
        "# Step 1: Pretrain RBMs\n",
        "dbn.pretrain(train_loader, epochs=EPOCHS_PRETRAIN)\n",
        "\n",
        "# Step 2: Fine-tune with supervision\n",
        "train_supervised(dbn, train_loader, epochs=EPOCHS_FINETUNE)\n",
        "\n",
        "# Step 3: Evaluate\n",
        "print(\"\\n[Evaluation on Test Set]\")\n",
        "evaluate(dbn, test_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BWPuFQtXpuZ",
        "outputId": "5eaff4eb-1932-40cd-c165-2b389d664da3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pretraining RBM Layer 1\n",
            "Pretraining RBM Layer 2\n",
            "Pretraining RBM Layer 3\n",
            "Epoch 1/5, Loss: 1794.7881\n",
            "Epoch 2/5, Loss: 1789.5091\n",
            "Epoch 3/5, Loss: 1790.2226\n",
            "Epoch 4/5, Loss: 1790.3169\n",
            "Epoch 5/5, Loss: 1789.5369\n",
            "[DBN] Accuracy on test set: 12.94%\n",
            "Epoch 1/10, Loss: 1463.4042\n",
            "Epoch 2/10, Loss: 1306.0873\n",
            "Epoch 3/10, Loss: 1209.6700\n",
            "Epoch 4/10, Loss: 1140.7519\n",
            "Epoch 5/10, Loss: 1077.3485\n",
            "Epoch 6/10, Loss: 1014.1018\n",
            "Epoch 7/10, Loss: 954.0838\n",
            "Epoch 8/10, Loss: 897.6212\n",
            "Epoch 9/10, Loss: 840.0668\n",
            "Epoch 10/10, Loss: 784.0294\n",
            "[DNN (baseline)] Accuracy on test set: 45.48%\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "45.48"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# -------------------------------- Setup -------------------------------- #\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "BATCH_SIZE = 64\n",
        "INPUT_SIZE = 32 * 32  # Grayscale image\n",
        "NUM_CLASSES = 10\n",
        "EPOCHS_PRETRAIN = 5\n",
        "EPOCHS_FINETUNE = 5\n",
        "\n",
        "# -------------------------- Load CIFAR-10 (Grayscale) -------------------------- #\n",
        "transform = transforms.Compose([\n",
        "    transforms.Grayscale(), transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "train_data = datasets.CIFAR10(root='./data', train=True, transform=transform, download=True)\n",
        "test_data = datasets.CIFAR10(root='./data', train=False, transform=transform, download=True)\n",
        "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE)\n",
        "\n",
        "# ----------------------------- RBM Definition ----------------------------- #\n",
        "class RBM(nn.Module):\n",
        "    def __init__(self, n_vis, n_hid):\n",
        "        super(RBM, self).__init__()\n",
        "        self.W = nn.Parameter(torch.randn(n_hid, n_vis) * 0.01)\n",
        "        self.v_bias = nn.Parameter(torch.zeros(n_vis))\n",
        "        self.h_bias = nn.Parameter(torch.zeros(n_hid))\n",
        "\n",
        "    def sample_h(self, v):\n",
        "        prob = torch.sigmoid(F.linear(v, self.W, self.h_bias))\n",
        "        return prob.bernoulli(), prob\n",
        "\n",
        "    def sample_v(self, h):\n",
        "        prob = torch.sigmoid(F.linear(h, self.W.t(), self.v_bias))\n",
        "        return prob.bernoulli(), prob\n",
        "\n",
        "    def contrastive_divergence(self, v0, lr=0.1):\n",
        "        vk = v0\n",
        "        for _ in range(1):  # CD-1\n",
        "            hk, _ = self.sample_h(vk)\n",
        "            vk, _ = self.sample_v(hk)\n",
        "        h0 = torch.sigmoid(F.linear(v0, self.W, self.h_bias))\n",
        "        hk = torch.sigmoid(F.linear(vk, self.W, self.h_bias))\n",
        "        self.W.data += lr * (h0.t() @ v0 - hk.t() @ vk) / v0.size(0)\n",
        "        self.v_bias.data += lr * torch.mean(v0 - vk, dim=0)\n",
        "        self.h_bias.data += lr * torch.mean(h0 - hk, dim=0)\n",
        "\n",
        "# --------------------------- DBN Model --------------------------- #\n",
        "class DBN(nn.Module):\n",
        "    def __init__(self, layer_sizes):\n",
        "        super(DBN, self).__init__()\n",
        "        self.rbms = nn.ModuleList([RBM(layer_sizes[i], layer_sizes[i+1]) for i in range(len(layer_sizes)-1)])\n",
        "        self.classifier = nn.Sequential(nn.Linear(layer_sizes[-1], NUM_CLASSES))\n",
        "\n",
        "    def pretrain(self, loader, epochs):\n",
        "        data = []\n",
        "        for idx, rbm in enumerate(self.rbms):\n",
        "            print(f\"Pretraining RBM Layer {idx+1}\")\n",
        "            for epoch in range(epochs):\n",
        "                for x, _ in loader:\n",
        "                    x = x.view(x.size(0), -1).to(device)\n",
        "                    for prev_rbm in data:\n",
        "                        x = torch.sigmoid(F.linear(x, prev_rbm.W, prev_rbm.h_bias))\n",
        "                    rbm.contrastive_divergence(x)\n",
        "            data.append(rbm)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        for rbm in self.rbms:\n",
        "            x = torch.sigmoid(F.linear(x, rbm.W, rbm.h_bias))\n",
        "        return self.classifier(x)\n",
        "\n",
        "# --------------------------- DNN Model --------------------------- #\n",
        "class DNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DNN, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(INPUT_SIZE, 512), nn.ReLU(),\n",
        "            nn.Linear(512, 256), nn.ReLU(),\n",
        "            nn.Linear(256, 128), nn.ReLU(),\n",
        "            nn.Linear(128, NUM_CLASSES)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# -------------------- Training and Evaluation -------------------- #\n",
        "def train(model, loader, epochs):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            pred = model(x)\n",
        "            loss = loss_fn(pred, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "def evaluate(model, loader, name=\"Model\"):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            preds = model(x).argmax(dim=1)\n",
        "            correct += (preds == y).sum().item()\n",
        "            total += y.size(0)\n",
        "    acc = 100 * correct / total\n",
        "    print(f\"[{name}] Accuracy on test set: {acc:.2f}%\")\n",
        "    return acc\n",
        "\n",
        "# ---------------------------- Run ---------------------------- #\n",
        "layer_sizes = [INPUT_SIZE, 512, 256, 128]\n",
        "dbn = DBN(layer_sizes)\n",
        "dnn = DNN()\n",
        "\n",
        "# DBN Training\n",
        "dbn.pretrain(train_loader, epochs=EPOCHS_PRETRAIN)\n",
        "train(dbn, train_loader, epochs=EPOCHS_FINETUNE)\n",
        "evaluate(dbn, test_loader, \"DBN\")\n",
        "\n",
        "# DNN Training\n",
        "train(dnn, train_loader, epochs=EPOCHS_FINETUNE + EPOCHS_PRETRAIN)  # Equal total epochs\n",
        "evaluate(dnn, test_loader, \"DNN (baseline)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wtha__hWnJQB"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}